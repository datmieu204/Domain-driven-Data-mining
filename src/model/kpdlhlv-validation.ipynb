{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install keybert","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!huggingface-cli login --token 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","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  TEx","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom datasets import load_dataset\nimport torch\nimport gc\nfrom tqdm import tqdm  # Import tqdm\nimport random\nimport numpy as np\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # for GPUs\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Example usage\nset_seed(1510)\n\ncheckpoint = \"lengocquangLAB/t5-small-tags-extraction-800\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n\nmodel.cuda()\n\n# Tải dataset 'vie-news-tags-extraction' từ Hugging Face\ndataset = load_dataset(\"lengocquangLAB/vie-news-tags-extraction\")\ndocuments = dataset['validation']['content'][:100]\ntrue_tags = dataset['validation']['tags'][:100]\n\ncandidates = []\n\nbatch_size = 128\n\n# Wrap the loop with tqdm to track progress\nfor i in tqdm(range(0, len(documents), batch_size), desc=\"TEx is running\"):\n    sentences = documents[i:i+batch_size]\n    \n    # Encoding the input sentences\n    encoding = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length = 800)\n    input_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n    \n    # Generate model outputs\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_masks,\n            max_length=90,\n            early_stopping=True\n        ).to('cpu')\n        \n    for output in outputs:\n        line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        temp = line.split(', ')  # Chia theo dấu phẩy\n        temp = list(set(temp))  # Chuyển thành set rồi quay lại thành list để loại bỏ phần tử trùng\n        candidates.append(temp)\n\n        del line, temp\n        gc.collect()  # Explicit garbage collection\n        torch.cuda.empty_cache() \n\n    del encoding, sentences, input_ids, attention_masks, outputs\n    gc.collect()\n    torch.cuda.empty_cache()\n\ndel tokenizer, model\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n\n# Tạo DataFrame từ danh sách sentences và merged\ndf = pd.DataFrame({\n    'content': documents,\n    'pred_tags': [','.join(sublist) for sublist in candidates],\n    'true_tags': true_tags\n})\n\n# Xuất DataFrame ra file CSV với mã hóa UTF-8\ndf.to_csv('val_onlyTEx.csv', index=False, encoding='utf-8')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport ast\n\n# Convert the columns to list of tags\ntrue_tags = df['true_tags'].apply(ast.literal_eval).tolist()\npred_tags = df['pred_tags'].apply(lambda x: x.split(',')).tolist()\n\n# Initialize counters for TP, FP, FN, TN\nTP = FP = FN = TN = 0\n\n# Loop through each sample\nfor true_set, pred_set in zip(true_tags, pred_tags):\n    # Loop through each tag in true_set\n    print(\"This is true set\")\n    print(true_set)\n    print(\"This is pred set\")\n    print(pred_set)\n    for tag in true_set:\n        if tag in pred_set:\n            print(tag)\n            TP += 1  # True Positive: tag is in both true and predicted\n        else:\n            FN += 1  # False Negative: tag is in true but not in predicted\n            \n    # Loop through each tag in pred_set\n    for tag in pred_set:\n        if tag not in true_set:\n            FP += 1  # False Positive: tag is in predicted but not in true\n\n# Calculate Precision, Recall, and Micro F1 score\nmicro_precision = TP / (TP + FP) if (TP + FP) != 0 else 0\nmicro_recall = TP / (TP + FN) if (TP + FN) != 0 else 0\nmicro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) != 0 else 0\n\n# Print results\nprint(f\"True Positives (TP): {TP}\")\nprint(f\"False Positives (FP): {FP}\")\nprint(f\"False Negatives (FN): {FN}\")\nprint(f\"True Negatives (TN): {TN}\")  # This will be 0 unless you define a universal set of tags\nprint(f\"Micro F1 Score: {micro_f1}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# keyLLM","metadata":{}},{"cell_type":"code","source":"from torch import cuda, bfloat16\nimport transformers\n\nmodel_id = 'Qwen/Qwen2.5-1.5B-Instruct'\n\n# Llama 2 Model & Tokenizer\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    device_map='auto',\n)\nmodel.eval()\n\n# Our text generator\ngenerator = transformers.pipeline(\n    model=model, tokenizer=tokenizer,\n    task='text-generation',\n    temperature=0.1,\n    max_new_tokens=500,\n    repetition_penalty=1.1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keybert.llm import TextGeneration\nfrom keybert import KeyLLM\n\nprompt = \"\"\"\n<s>[INST] <<SYS>>\n\nBạn là một trợ lý hữu ích chuyên trích xuất 20 từ khóa khác nhau phân cách bằng dấu phẩy từ văn bản. \nBạn cần trả lời trực tiếp và chỉ đưa ra 20 từ khóa khác nhau mà không có bất kỳ lời giải thích hay thông tin thừa nào. \n\n<</SYS>> \nTôi có tài liệu sau:\n- Trang web nói rằng chỉ mất vài ngày để giao hàng nhưng tôi vẫn chưa nhận được đơn hàng của mình.\n\nVui lòng cho tôi biết các từ khóa có trong tài liệu này và phân tách chúng bằng dấu phẩy. \nHãy chắc chắn rằng bạn chỉ trả về các từ khóa mà không nói gì thêm. \nVí dụ, đừng nói: \"Đây là các từ khóa có trong tài liệu\" [/INST] thịt, bò, ăn, ăn uống, khí thải, bít tết, thực phẩm, sức khỏe, chế biến, gà [INST]]\n\nTôi có tài liệu sau:\n\n- [DOCUMENT]\n\nVới các từ khoá gợi ý:\n[CANDIDATES]\n\n\nDựa trên các từ khoá gợi ý ở phía trên vui lòng cho tôi biết 20 từ khóa khác nhau có trong tài liệu này và phân tách chúng bằng dấu phẩy. \nHãy chắc chắn rằng bạn chỉ trả về các từ khóa mà không nói gì thêm. \nVí dụ, đừng nói: \"Đây là các từ khóa có trong tài liệu\" \n[/INST]\n\"\"\"\n\n# Load it in KeyLLM\nllm = TextGeneration(generator, prompt=prompt)\nkw_model = KeyLLM(llm)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged = []\nmax_tags = 19\n\n# Wrap the loop with tqdm to show progress\nfor i in tqdm(range(0, len(documents), batch_size), desc=\"keyLLM is running\"):\n    sentences = documents[i:i+batch_size]\n    candidate_keywords = candidates[i:i+batch_size]\n    \n    # Extract keywords using the kw_model\n    keywords = kw_model.extract_keywords(sentences, candidate_keywords=candidate_keywords) \n    \n    for j in range(len(keywords)):\n        # Remove words longer than 15 characters, duplicates, empty strings, and words containing '\\n' or '[s]'\n        keywords[j] = list(set(word for word in keywords[j] \n                               if len(word) < 15 and word != \"\" and '\\n' not in word and '[s]' not in word))\n    \n    # Merge candidate_keywords with the newly extracted keywords\n    for j in range(len(keywords)):\n        # Start with the candidate keywords\n        merged_sublist = candidate_keywords[j]\n        \n        # If there are fewer than 20 words, add from keywords\n        if len(merged_sublist) < max_tags:\n            # Filter words that are not already in merged_sublist and remove empty strings\n            remaining_words = [word for word in keywords[j] if word not in merged_sublist and word != \"\"]\n            # Add enough to reach 20\n            merged_sublist.extend(remaining_words[:max_tags - len(merged_sublist)])\n        \n        # Ensure no more than 20 words in the list\n        merged_sublist = merged_sublist[:max_tags]\n        \n        # Add the merged sublist to the final list\n        merged.append(merged_sublist)\n\n# Convert each sublist into a comma-separated string\nmerged = [','.join(sublist) for sublist in merged]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Export files","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n\n# Tạo DataFrame từ danh sách sentences và merged\ndf = pd.DataFrame({\n    'content': documents,\n    'pred_tags': merged,\n    'true_tags': true_tags\n})\n\n# Xuất DataFrame ra file CSV với mã hóa UTF-8\ndf.to_csv('val_TEx_keyLLM.csv', index=False, encoding='utf-8')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['pred_tags']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport ast\n\n# Convert the columns to list of tags\n# Check the first few rows to verify the column type\nprint(type(df['true_tags'].iloc[0]))  # Check if it's a string or list\n\n# If it's a string representation of a list, use ast.literal_eval\nif isinstance(df['true_tags'].iloc[0], str):\n    import ast\n    true_tags = df['true_tags'].apply(ast.literal_eval).tolist()\nelse:\n    true_tags = df['true_tags'].tolist()\n    \npred_tags = df['pred_tags'].apply(lambda x: x.split(',')).tolist()\n\n# Initialize counters for TP, FP, FN, TN\nTP = FP = FN = TN = 0\n\n# Loop through each sample\nfor true_set, pred_set in zip(true_tags, pred_tags):\n    # Loop through each tag in true_set\n    print(\"This is true set\")\n    print(true_set)\n    print(\"This is pred set\")\n    print(pred_set)\n    for tag in true_set:\n        if tag in pred_set:\n            print(tag)\n            TP += 1  # True Positive: tag is in both true and predicted\n        else:\n            FN += 1  # False Negative: tag is in true but not in predicted\n            \n    # Loop through each tag in pred_set\n    for tag in pred_set:\n        if tag not in true_set:\n            FP += 1  # False Positive: tag is in predicted but not in true\n\n# Calculate Precision, Recall, and Micro F1 score\nmicro_precision = TP / (TP + FP) if (TP + FP) != 0 else 0\nmicro_recall = TP / (TP + FN) if (TP + FN) != 0 else 0\nmicro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) != 0 else 0\n\n# Print results\nprint(f\"True Positives (TP): {TP}\")\nprint(f\"False Positives (FP): {FP}\")\nprint(f\"False Negatives (FN): {FN}\")\nprint(f\"True Negatives (TN): {TN}\")  # This will be 0 unless you define a universal set of tags\nprint(f\"Micro F1 Score: {micro_f1}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}