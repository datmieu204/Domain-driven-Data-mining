{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(1510)\n",
    "\n",
    "checkpoint = \"lengocquangLAB/t5-small-tags-extraction-800\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "dataset = load_dataset(\"lengocquangLAB/vie-news-tags-extraction\")\n",
    "documents = dataset['validation']['content'][:100]\n",
    "true_tags = dataset['validation']['tags'][:100]\n",
    "\n",
    "candidates = []\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "for i in tqdm(range(0, len(documents), batch_size), desc=\"TEx is running\"):\n",
    "    sentences = documents[i:i+batch_size]\n",
    "    \n",
    "    encoding = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length = 800)\n",
    "    input_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_masks,\n",
    "            max_length=90,\n",
    "            early_stopping=True\n",
    "        ).to('cpu')\n",
    "        \n",
    "    for output in outputs:\n",
    "        line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        temp = line.split(', ')  \n",
    "        temp = list(set(temp))  \n",
    "        candidates.append(temp)\n",
    "\n",
    "        del line, temp\n",
    "        gc.collect()  \n",
    "        torch.cuda.empty_cache() \n",
    "\n",
    "    del encoding, sentences, input_ids, attention_masks, outputs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "del tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'content': documents,\n",
    "    'pred_tags': [','.join(sublist) for sublist in candidates],\n",
    "    'true_tags': true_tags\n",
    "})\n",
    "\n",
    "df.to_csv('val_onlyTEx.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "true_tags = df['true_tags'].apply(ast.literal_eval).tolist()\n",
    "pred_tags = df['pred_tags'].apply(lambda x: x.split(',')).tolist()\n",
    "\n",
    "TP = FP = FN = TN = 0\n",
    "\n",
    "for true_set, pred_set in zip(true_tags, pred_tags):\n",
    "    print(\"This is true set\")\n",
    "    print(true_set)\n",
    "    print(\"This is pred set\")\n",
    "    print(pred_set)\n",
    "    for tag in true_set:\n",
    "        if tag in pred_set:\n",
    "            print(tag)\n",
    "            TP += 1 \n",
    "        else:\n",
    "            FN += 1  \n",
    "            \n",
    "    for tag in pred_set:\n",
    "        if tag not in true_set:\n",
    "            FP += 1  \n",
    "\n",
    "micro_precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "micro_recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) != 0 else 0\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(f\"True Negatives (TN): {TN}\")  \n",
    "print(f\"Micro F1 Score: {micro_f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keyLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "\n",
    "# Llama 2 Model & Tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto',\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Our text generator\n",
    "generator = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=500,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from keybert.llm import TextGeneration\n",
    "from keybert import KeyLLM\n",
    "\n",
    "prompt = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "\n",
    "Bạn là một trợ lý hữu ích chuyên trích xuất 20 từ khóa khác nhau phân cách bằng dấu phẩy từ văn bản. \n",
    "Bạn cần trả lời trực tiếp và chỉ đưa ra 20 từ khóa khác nhau mà không có bất kỳ lời giải thích hay thông tin thừa nào. \n",
    "\n",
    "<</SYS>> \n",
    "Tôi có tài liệu sau:\n",
    "- Trang web nói rằng chỉ mất vài ngày để giao hàng nhưng tôi vẫn chưa nhận được đơn hàng của mình.\n",
    "\n",
    "Vui lòng cho tôi biết các từ khóa có trong tài liệu này và phân tách chúng bằng dấu phẩy. \n",
    "Hãy chắc chắn rằng bạn chỉ trả về các từ khóa mà không nói gì thêm. \n",
    "Ví dụ, đừng nói: \"Đây là các từ khóa có trong tài liệu\" [/INST] thịt, bò, ăn, ăn uống, khí thải, bít tết, thực phẩm, sức khỏe, chế biến, gà [INST]]\n",
    "\n",
    "Tôi có tài liệu sau:\n",
    "\n",
    "- [DOCUMENT]\n",
    "\n",
    "Với các từ khoá gợi ý:\n",
    "[CANDIDATES]\n",
    "\n",
    "\n",
    "Dựa trên các từ khoá gợi ý ở phía trên vui lòng cho tôi biết 20 từ khóa khác nhau có trong tài liệu này và phân tách chúng bằng dấu phẩy. \n",
    "Hãy chắc chắn rằng bạn chỉ trả về các từ khóa mà không nói gì thêm. \n",
    "Ví dụ, đừng nói: \"Đây là các từ khóa có trong tài liệu\" \n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "# Load it in KeyLLM\n",
    "llm = TextGeneration(generator, prompt=prompt)\n",
    "kw_model = KeyLLM(llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged = []\n",
    "max_tags = 19\n",
    "\n",
    "# Wrap the loop with tqdm to show progress\n",
    "for i in tqdm(range(0, len(documents), batch_size), desc=\"keyLLM is running\"):\n",
    "    sentences = documents[i:i+batch_size]\n",
    "    candidate_keywords = candidates[i:i+batch_size]\n",
    "    \n",
    "    # Extract keywords using the kw_model\n",
    "    keywords = kw_model.extract_keywords(sentences, candidate_keywords=candidate_keywords) \n",
    "    \n",
    "    for j in range(len(keywords)):\n",
    "        # Remove words longer than 15 characters, duplicates, empty strings, and words containing '\\n' or '[s]'\n",
    "        keywords[j] = list(set(word for word in keywords[j] \n",
    "                               if len(word) < 15 and word != \"\" and '\\n' not in word and '[s]' not in word))\n",
    "    \n",
    "    # Merge candidate_keywords with the newly extracted keywords\n",
    "    for j in range(len(keywords)):\n",
    "        # Start with the candidate keywords\n",
    "        merged_sublist = candidate_keywords[j]\n",
    "        \n",
    "        # If there are fewer than 20 words, add from keywords\n",
    "        if len(merged_sublist) < max_tags:\n",
    "            # Filter words that are not already in merged_sublist and remove empty strings\n",
    "            remaining_words = [word for word in keywords[j] if word not in merged_sublist and word != \"\"]\n",
    "            # Add enough to reach 20\n",
    "            merged_sublist.extend(remaining_words[:max_tags - len(merged_sublist)])\n",
    "        \n",
    "        # Ensure no more than 20 words in the list\n",
    "        merged_sublist = merged_sublist[:max_tags]\n",
    "        \n",
    "        # Add the merged sublist to the final list\n",
    "        merged.append(merged_sublist)\n",
    "\n",
    "# Convert each sublist into a comma-separated string\n",
    "merged = [','.join(sublist) for sublist in merged]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Tạo DataFrame từ danh sách sentences và merged\n",
    "df = pd.DataFrame({\n",
    "    'content': documents,\n",
    "    'pred_tags': merged,\n",
    "    'true_tags': true_tags\n",
    "})\n",
    "\n",
    "# Xuất DataFrame ra file CSV với mã hóa UTF-8\n",
    "df.to_csv('val_TEx_keyLLM.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['pred_tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Convert the columns to list of tags\n",
    "# Check the first few rows to verify the column type\n",
    "print(type(df['true_tags'].iloc[0]))  # Check if it's a string or list\n",
    "\n",
    "# If it's a string representation of a list, use ast.literal_eval\n",
    "if isinstance(df['true_tags'].iloc[0], str):\n",
    "    import ast\n",
    "    true_tags = df['true_tags'].apply(ast.literal_eval).tolist()\n",
    "else:\n",
    "    true_tags = df['true_tags'].tolist()\n",
    "    \n",
    "pred_tags = df['pred_tags'].apply(lambda x: x.split(',')).tolist()\n",
    "\n",
    "# Initialize counters for TP, FP, FN, TN\n",
    "TP = FP = FN = TN = 0\n",
    "\n",
    "# Loop through each sample\n",
    "for true_set, pred_set in zip(true_tags, pred_tags):\n",
    "    # Loop through each tag in true_set\n",
    "    print(\"This is true set\")\n",
    "    print(true_set)\n",
    "    print(\"This is pred set\")\n",
    "    print(pred_set)\n",
    "    for tag in true_set:\n",
    "        if tag in pred_set:\n",
    "            print(tag)\n",
    "            TP += 1  # True Positive: tag is in both true and predicted\n",
    "        else:\n",
    "            FN += 1  # False Negative: tag is in true but not in predicted\n",
    "            \n",
    "    # Loop through each tag in pred_set\n",
    "    for tag in pred_set:\n",
    "        if tag not in true_set:\n",
    "            FP += 1  # False Positive: tag is in predicted but not in true\n",
    "\n",
    "# Calculate Precision, Recall, and Micro F1 score\n",
    "micro_precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "micro_recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) != 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(f\"True Negatives (TN): {TN}\")  # This will be 0 unless you define a universal set of tags\n",
    "print(f\"Micro F1 Score: {micro_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
